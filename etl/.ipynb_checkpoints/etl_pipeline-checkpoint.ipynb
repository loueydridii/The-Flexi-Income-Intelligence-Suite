{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ETL Pipeline - Freelance Job Earnings Data Warehouse\n",
                "\n",
                "This notebook extracts data from raw sources, transforms it into a star schema, and loads it into dimension and fact tables.\n",
                "\n",
                "## Output Tables:\n",
                "- **fact_job_earnings.csv** - Fact table with job earnings metrics\n",
                "- **dim_worker.csv** - Worker dimension\n",
                "- **dim_platform.csv** - Platform dimension\n",
                "- **dim_region.csv** - Region dimension\n",
                "- **dim_project.csv** - Project dimension\n",
                "- **dim_date.csv** - Date dimension"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import json\n",
                "import os\n",
                "from pathlib import Path"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define paths\n",
                "BASE_DIR = Path.cwd().parent\n",
                "RAW_DATA_DIR = BASE_DIR / 'data_raw'\n",
                "CLEANED_DATA_DIR = BASE_DIR / 'data_cleaned'\n",
                "\n",
                "# Create output directory if it doesn't exist\n",
                "CLEANED_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "print(f\"Raw data directory: {RAW_DATA_DIR}\")\n",
                "print(f\"Cleaned data directory: {CLEANED_DATA_DIR}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 2. EXTRACT - Load Raw Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Extract jobs transactions (main facts data)\n",
                "jobs_df = pd.read_csv(RAW_DATA_DIR / 'jobs_transactions.csv')\n",
                "print(f\"Jobs transactions: {len(jobs_df)} records\")\n",
                "jobs_df.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Extract workers dimension (JSON)\n",
                "with open(RAW_DATA_DIR / 'workers.json', 'r') as f:\n",
                "    workers_raw = json.load(f)\n",
                "workers_df = pd.DataFrame(workers_raw)\n",
                "print(f\"Workers: {len(workers_df)} records\")\n",
                "workers_df.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Extract platforms dimension (CSV)\n",
                "platforms_df = pd.read_csv(RAW_DATA_DIR / 'platforms.csv')\n",
                "print(f\"Platforms: {len(platforms_df)} records\")\n",
                "platforms_df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Extract regions dimension (JSON)\n",
                "with open(RAW_DATA_DIR / 'regions.json', 'r') as f:\n",
                "    regions_raw = json.load(f)\n",
                "regions_df = pd.DataFrame(regions_raw)\n",
                "print(f\"Regions: {len(regions_df)} records\")\n",
                "regions_df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Extract projects dimension (CSV)\n",
                "projects_df = pd.read_csv(RAW_DATA_DIR / 'projects.csv')\n",
                "print(f\"Projects: {len(projects_df)} records\")\n",
                "projects_df.head(10)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Extract date dimension (CSV)\n",
                "dim_date_df = pd.read_csv(RAW_DATA_DIR / 'dim_date.csv')\n",
                "print(f\"Dates: {len(dim_date_df)} records\")\n",
                "dim_date_df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 3. TRANSFORM - Create Dimension Tables"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.1 dim_worker"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Transform worker dimension\n",
                "dim_worker = workers_df.copy()\n",
                "\n",
                "# Ensure proper column order as per data dictionary\n",
                "dim_worker = dim_worker[['worker_id', 'experience_level', 'primary_skill']]\n",
                "\n",
                "# Convert worker_id to string (as per data dictionary)\n",
                "dim_worker['worker_id'] = dim_worker['worker_id'].astype(str)\n",
                "\n",
                "print(f\"dim_worker shape: {dim_worker.shape}\")\n",
                "dim_worker.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.2 dim_platform"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Transform platform dimension\n",
                "dim_platform = platforms_df.copy()\n",
                "\n",
                "# Standardize platform names for matching with jobs data\n",
                "# Create a mapping for case-insensitive matching\n",
                "dim_platform['platform_name_standardized'] = dim_platform['platform_name'].str.title()\n",
                "\n",
                "# Rename to match data dictionary\n",
                "dim_platform = dim_platform.rename(columns={'payment_cycle': 'payment_cycle'})\n",
                "\n",
                "# Ensure proper column order\n",
                "dim_platform = dim_platform[['platform_id', 'platform_name', 'category', 'payment_cycle']]\n",
                "\n",
                "print(f\"dim_platform shape: {dim_platform.shape}\")\n",
                "dim_platform"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.3 dim_region"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Transform region dimension\n",
                "dim_region = regions_df.copy()\n",
                "\n",
                "# Ensure proper column order as per data dictionary\n",
                "dim_region = dim_region[['region_id', 'region', 'cost_of_living_index']]\n",
                "\n",
                "print(f\"dim_region shape: {dim_region.shape}\")\n",
                "dim_region"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.4 dim_project"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Transform project dimension\n",
                "dim_project = projects_df.copy()\n",
                "\n",
                "# Ensure proper column order as per data dictionary\n",
                "dim_project = dim_project[['project_id', 'project_type', 'job_category']]\n",
                "\n",
                "print(f\"dim_project shape: {dim_project.shape}\")\n",
                "dim_project"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.5 dim_date"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Transform date dimension\n",
                "dim_date = dim_date_df.copy()\n",
                "\n",
                "# Ensure proper column order as per data dictionary\n",
                "dim_date = dim_date[['date_id', 'full_date', 'day_of_week', 'is_weekend', 'is_holiday', \n",
                "                      'month_name', 'month_number', 'quarter', 'year']]\n",
                "\n",
                "print(f\"dim_date shape: {dim_date.shape}\")\n",
                "dim_date.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 4. TRANSFORM - Create Fact Table"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Start with jobs transactions\n",
                "fact_job_earnings = jobs_df.copy()\n",
                "\n",
                "# Convert work_date to datetime for joining\n",
                "fact_job_earnings['work_date'] = pd.to_datetime(fact_job_earnings['work_date'])\n",
                "\n",
                "print(f\"Initial fact table: {len(fact_job_earnings)} records\")\n",
                "fact_job_earnings.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create date_id from work_date (YYYYMMDD format)\n",
                "fact_job_earnings['date_id'] = fact_job_earnings['work_date'].dt.strftime('%Y%m%d').astype(int)\n",
                "\n",
                "fact_job_earnings[['work_date', 'date_id']].head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create platform lookup for getting platform_id\n",
                "# Handle case differences (e.g., 'PeoplePerHour' vs 'Peopleperhour')\n",
                "platform_lookup = dim_platform[['platform_id', 'platform_name']].copy()\n",
                "platform_lookup['platform_name_lower'] = platform_lookup['platform_name'].str.lower()\n",
                "\n",
                "fact_job_earnings['platform_lower'] = fact_job_earnings['platform'].str.lower()\n",
                "\n",
                "# Merge to get platform_id\n",
                "fact_job_earnings = fact_job_earnings.merge(\n",
                "    platform_lookup[['platform_id', 'platform_name_lower']],\n",
                "    left_on='platform_lower',\n",
                "    right_on='platform_name_lower',\n",
                "    how='left'\n",
                ")\n",
                "\n",
                "# Drop temporary columns\n",
                "fact_job_earnings = fact_job_earnings.drop(columns=['platform_lower', 'platform_name_lower'])\n",
                "\n",
                "print(f\"Platform ID mapping check (null count): {fact_job_earnings['platform_id'].isna().sum()}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create region lookup for getting region_id\n",
                "# Handle case differences (e.g., 'UK' vs 'Uk', 'USA' vs 'Usa')\n",
                "region_lookup = dim_region[['region_id', 'region']].copy()\n",
                "region_lookup['region_lower'] = region_lookup['region'].str.lower()\n",
                "\n",
                "fact_job_earnings['client_region_lower'] = fact_job_earnings['client_region'].str.lower()\n",
                "\n",
                "# Merge to get region_id\n",
                "fact_job_earnings = fact_job_earnings.merge(\n",
                "    region_lookup[['region_id', 'region_lower']],\n",
                "    left_on='client_region_lower',\n",
                "    right_on='region_lower',\n",
                "    how='left'\n",
                ")\n",
                "\n",
                "# Drop temporary columns\n",
                "fact_job_earnings = fact_job_earnings.drop(columns=['client_region_lower', 'region_lower'])\n",
                "\n",
                "print(f\"Region ID mapping check (null count): {fact_job_earnings['region_id'].isna().sum()}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create project lookup for getting project_id\n",
                "# Need to match on both project_type and job_category\n",
                "project_lookup = dim_project[['project_id', 'project_type', 'job_category']].copy()\n",
                "project_lookup['project_type_lower'] = project_lookup['project_type'].str.lower()\n",
                "project_lookup['job_category_lower'] = project_lookup['job_category'].str.lower()\n",
                "\n",
                "fact_job_earnings['project_type_lower'] = fact_job_earnings['project_type'].str.lower()\n",
                "fact_job_earnings['job_category_lower'] = fact_job_earnings['job_category'].str.lower()\n",
                "\n",
                "# Merge to get project_id\n",
                "fact_job_earnings = fact_job_earnings.merge(\n",
                "    project_lookup[['project_id', 'project_type_lower', 'job_category_lower']],\n",
                "    on=['project_type_lower', 'job_category_lower'],\n",
                "    how='left'\n",
                ")\n",
                "\n",
                "# Drop temporary columns\n",
                "fact_job_earnings = fact_job_earnings.drop(columns=['project_type_lower', 'job_category_lower'])\n",
                "\n",
                "print(f\"Project ID mapping check (null count): {fact_job_earnings['project_id'].isna().sum()}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Convert worker_id to string for consistency with dim_worker\n",
                "fact_job_earnings['worker_id'] = fact_job_earnings['worker_id'].astype(str)\n",
                "\n",
                "# Create is_gap_day flag: 1 if earnings are zero/missing or job_completed is 0, else 0\n",
                "fact_job_earnings['is_gap_day'] = (\n",
                "    (fact_job_earnings['earnings_usd'].isna()) | \n",
                "    (fact_job_earnings['earnings_usd'] == 0) | \n",
                "    (fact_job_earnings['job_completed'] == 0)\n",
                ").astype(int)\n",
                "\n",
                "print(f\"is_gap_day distribution:\\n{fact_job_earnings['is_gap_day'].value_counts()}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Select and order columns for final fact table as per data dictionary\n",
                "fact_columns = [\n",
                "    'job_id',           # DK - Degenerate key\n",
                "    'worker_id',        # FK -> dim_worker\n",
                "    'platform_id',      # FK -> dim_platform\n",
                "    'region_id',        # FK -> dim_region\n",
                "    'project_id',       # FK -> dim_project\n",
                "    'date_id',          # FK -> dim_date\n",
                "    'earnings_usd',     # Metric\n",
                "    'job_completed',    # Metric\n",
                "    'job_duration_days',# Metric\n",
                "    'hourly_rate',      # Metric\n",
                "    'job_success_rate', # Metric\n",
                "    'client_rating',    # Metric\n",
                "    'rehire_rate',      # Metric\n",
                "    'marketing_spend',  # Metric\n",
                "    'is_gap_day'        # Calculated flag\n",
                "]\n",
                "\n",
                "fact_job_earnings_final = fact_job_earnings[fact_columns].copy()\n",
                "\n",
                "# Convert FK columns to int (handling NaN)\n",
                "for col in ['platform_id', 'region_id', 'project_id']:\n",
                "    fact_job_earnings_final[col] = fact_job_earnings_final[col].fillna(-1).astype(int)\n",
                "    # Replace -1 back to NaN for cleaner output (optional)\n",
                "    fact_job_earnings_final[col] = fact_job_earnings_final[col].replace(-1, pd.NA)\n",
                "\n",
                "print(f\"Final fact_job_earnings shape: {fact_job_earnings_final.shape}\")\n",
                "fact_job_earnings_final.head(10)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 5. Data Quality Checks"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check for any unmapped foreign keys\n",
                "print(\"=== Data Quality Report ===\")\n",
                "print(f\"\\nFact Table Records: {len(fact_job_earnings_final)}\")\n",
                "print(f\"\\nNull/Unmapped Foreign Keys:\")\n",
                "print(f\"  - platform_id: {fact_job_earnings_final['platform_id'].isna().sum()}\")\n",
                "print(f\"  - region_id: {fact_job_earnings_final['region_id'].isna().sum()}\")\n",
                "print(f\"  - project_id: {fact_job_earnings_final['project_id'].isna().sum()}\")\n",
                "print(f\"\\nDimension Tables:\")\n",
                "print(f\"  - dim_worker: {len(dim_worker)} records\")\n",
                "print(f\"  - dim_platform: {len(dim_platform)} records\")\n",
                "print(f\"  - dim_region: {len(dim_region)} records\")\n",
                "print(f\"  - dim_project: {len(dim_project)} records\")\n",
                "print(f\"  - dim_date: {len(dim_date)} records\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Verify referential integrity for worker_id\n",
                "workers_in_fact = set(fact_job_earnings_final['worker_id'].unique())\n",
                "workers_in_dim = set(dim_worker['worker_id'].unique())\n",
                "\n",
                "orphan_workers = workers_in_fact - workers_in_dim\n",
                "print(f\"Workers in fact but not in dimension: {len(orphan_workers)}\")\n",
                "if orphan_workers:\n",
                "    print(f\"  Sample orphan worker IDs: {list(orphan_workers)[:5]}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 6. LOAD - Save to data_cleaned folder"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save dimension tables\n",
                "dim_worker.to_csv(CLEANED_DATA_DIR / 'dim_worker.csv', index=False)\n",
                "print(f\"✓ Saved dim_worker.csv ({len(dim_worker)} records)\")\n",
                "\n",
                "dim_platform.to_csv(CLEANED_DATA_DIR / 'dim_platform.csv', index=False)\n",
                "print(f\"✓ Saved dim_platform.csv ({len(dim_platform)} records)\")\n",
                "\n",
                "dim_region.to_csv(CLEANED_DATA_DIR / 'dim_region.csv', index=False)\n",
                "print(f\"✓ Saved dim_region.csv ({len(dim_region)} records)\")\n",
                "\n",
                "dim_project.to_csv(CLEANED_DATA_DIR / 'dim_project.csv', index=False)\n",
                "print(f\"✓ Saved dim_project.csv ({len(dim_project)} records)\")\n",
                "\n",
                "dim_date.to_csv(CLEANED_DATA_DIR / 'dim_date.csv', index=False)\n",
                "print(f\"✓ Saved dim_date.csv ({len(dim_date)} records)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save fact table\n",
                "fact_job_earnings_final.to_csv(CLEANED_DATA_DIR / 'fact_job_earnings.csv', index=False)\n",
                "print(f\"✓ Saved fact_job_earnings.csv ({len(fact_job_earnings_final)} records)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# List all output files\n",
                "print(\"\\n=== ETL Pipeline Complete ===\")\n",
                "print(f\"\\nOutput files in {CLEANED_DATA_DIR}:\")\n",
                "for file in sorted(CLEANED_DATA_DIR.glob('*.csv')):\n",
                "    size = file.stat().st_size / 1024  # KB\n",
                "    print(f\"  - {file.name} ({size:.1f} KB)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 7. Summary Statistics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Display summary statistics for the fact table\n",
                "print(\"=== Fact Table Summary Statistics ===\")\n",
                "fact_job_earnings_final[['earnings_usd', 'job_completed', 'job_duration_days', \n",
                "                          'hourly_rate', 'job_success_rate', 'client_rating', \n",
                "                          'rehire_rate', 'marketing_spend']].describe()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}